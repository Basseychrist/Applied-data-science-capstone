# SpaceX Data Collection using Web Scraping

This project focuses on collecting historical Falcon 9 launch records by scraping data from a Wikipedia page. This method is useful when an API is not available or does not provide the specific historical data required.

## Files

-   `jupyter-labs-webscraping.ipynb`: The Jupyter Notebook containing the web scraping code.
-   `spacex_web_scraped.csv`: The output dataset generated by the notebook.

## Notebook Overview and Code Explanation

The notebook `jupyter-labs-webscraping.ipynb` follows a structured approach to extract data from the web:

### 1. Setup and Imports
The notebook starts by importing necessary libraries:
-   `sys`: For system-specific parameters.
-   `requests`: To send HTTP requests to the Wikipedia URL.
-   `BeautifulSoup` (from `bs4`): To parse the HTML content of the page.
-   `re`: For regular expression operations.
-   `unicodedata`: To normalize unicode characters.
-   `pandas`: To organize the data into a DataFrame.

### 2. Helper Functions
Several helper functions are defined to clean and extract specific pieces of information from the HTML table cells:
-   `date_time`: Extracts the date and time from a cell, removing newlines.
-   `booster_version`: Extracts the booster version, handling potential nested elements.
-   `landing_status`: Extracts the landing status text.
-   `get_mass`: Cleans the payload mass text, removing "kg" and handling non-breaking spaces.
-   `extract_column_from_header`: Cleans the table header text to get clean column names.

### 3. Requesting the Data
-   **Goal**: Get the HTML content of the Wikipedia page.
-   **Code**: Uses `requests.get(static_url)` to fetch the page content.
-   **Reason**: We need the raw HTML to parse it for data.

### 4. Parsing HTML with BeautifulSoup
-   **Goal**: Create a navigable tree structure from the raw HTML.
-   **Code**: `soup = BeautifulSoup(response.text, 'html.parser')`.
-   **Reason**: BeautifulSoup makes it easy to search for specific tags (like `<table>`, `<tr>`, `<td>`) and extract text.

### 5. Extracting Column Names
-   **Goal**: Determine the columns for our dataset.
-   **Code**: Finds the specific launch table and iterates through `<th>` (table header) elements using `extract_column_from_header`.
-   **Reason**: To create a structured dictionary and eventually a DataFrame with correct headers.

### 6. Parsing the Launch Table
-   **Goal**: Extract row-by-row launch data.
-   **How it works**:
    1.  Iterates through all tables to find the specific launch tables (identified by class `wikitable plainrowheaders collapsible`).
    2.  Iterates through each row (`<tr>`) in the table.
    3.  Checks if the row corresponds to a flight number (to skip headers or non-data rows).
    4.  Iterates through cells (`<td>`) to extract:
        -   **Flight Number**: From the header or first cell.
        -   **Date & Time**: Using the `date_time` helper.
        -   **Booster Version**: Using the `booster_version` helper.
        -   **Launch Site**: Extracting text from the anchor tag.
        -   **Payload**: Extracting text from the anchor tag.
        -   **Payload Mass**: Using the `get_mass` helper.
        -   **Orbit**: Extracting text.
        -   **Customer**: Extracting text (handling missing links).
        -   **Launch Outcome**: Extracting text.
        -   **Booster Landing**: Using the `landing_status` helper.
    5.  Appends each extracted value to the corresponding list in `launch_dict`.

### 7. Creating the DataFrame and Exporting
-   **Goal**: Convert the dictionary into a usable format and save it.
-   **Code**: `pd.DataFrame(launch_dict)` and `df.to_csv()`.
-   **Reason**: Pandas DataFrames are the standard for data analysis in Python, and CSV is a portable format for sharing the data.

## How to Run the Lab

1.  **Prerequisites**: Install the required libraries:
    ```bash
    pip install requests pandas beautifulsoup4
    ```
2.  **Open the Notebook**: Open `jupyter-labs-webscraping.ipynb` in your Jupyter environment.
3.  **Run Cells**: Execute the cells in order. The notebook will scrape the data and generate `spacex_web_scraped.csv` in the same directory.
